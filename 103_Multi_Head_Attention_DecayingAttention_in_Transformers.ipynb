{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwOc3KTKRfZ7z53mR/vICh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaobin675/Path_in_ML_model_training/blob/main/103_Multi_Head_Attention_DecayingAttention_in_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOEAH2jzbBBE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DecayingAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        '''\n",
        "        This is a Multi-Head Attention implementation.\n",
        "        It includes a linear layer (self.qkv) to project the input into multiple heads,\n",
        "        allowing the model to learn different decay patterns simultaneously.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        #The linear layers allow the model to learn that certain features are good for searching (Query),\n",
        "        #while others are good for being found (Key), and others are good for content (Value).\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        # Optional: learnable decay scale\n",
        "        self.gamma = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] # [B, num_heads, L, head_dim]\n",
        "\n",
        "        # 1. Compute Dot Product: q @ k^T\n",
        "        # Scaling by sqrt(d_k) is standard for stability\n",
        "        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # 2. Create the Decay Bias b = |i - j|\n",
        "        indices = torch.arange(L, device=x.device)\n",
        "        # Broadcast indices to create a [L, L] distance matrix\n",
        "        distance_matrix = torch.abs(indices.view(L, 1) - indices.view(1, L))\n",
        "\n",
        "        # 3. Apply Negative Bias (Decay)\n",
        "        # scores: [B, heads, L, L], distance_matrix: [L, L]\n",
        "        # We multiply by gamma to control decay strength\n",
        "        decay_bias = -self.gamma * distance_matrix.unsqueeze(0)\n",
        "        scores = scores + decay_bias\n",
        "\n",
        "        # 4. Softmax and Value aggregation\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, L, C)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume the model has a hidden dimension of 512\n",
        "batch_size = 32\n",
        "seq_len = 100\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "# Create the module instance\n",
        "decay_layer = DecayingAttention(embed_dim=d_model, num_heads=num_heads)\n",
        "\n",
        "# Create dummy input data (Random noise)\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# The module handles Q, K, V projection internally\n",
        "output = decay_layer(x)\n",
        "# Result: torch.Size([32, 100, 512]) -> Matches input shape\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lUVcIpODdvX7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}